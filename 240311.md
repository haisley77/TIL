240311 TIL
## 실습을 위한 크롤링. 상업적 이용하지 않음!!

## 크롤링 프로세스 

- 네이버 부동산에서는 부동산 정보 조회와 관련한 api를 제공하지 않습니다.
- 따라서 HTML DOM을 파싱해서 정보를 추출하는 크롤링 과정이 필수입니다.
- 네이버 부동산 검색 키워드를 활용해 부동산 크롤링 URL 파라미터를 구성할 수 있습니다.
1. 필요한 파라미터 확인
2. 필요한 파라미터 수집
3. 파라미터 재구성 & URL 호출
    
    ```jsx
    _tradTpCd = [{tagCd: 'A1', uiTagNm: '매매'}, 
                 {tagCd: 'B1', uiTagNm: '전세'},  
                 {tagCd: 'B2', uiTagNm: '월세'},
                 {tagCd: 'B3', uiTagNm: '단기임대'}];
    _rletTpCd = [{tagCd: 'APT', uiTagNm: '아파트'}, {tagCd: 'OPST', uiTagNm: '오피스텔'}, {tagCd: 'VL', uiTagNm: '빌라'},
                 {tagCd: 'ABYG', uiTagNm: '아파트분양권'}, {tagCd: 'OBYG', uiTagNm: '오피스텔분양권'}, {tagCd: 'JGC', uiTagNm: '재건축'},
                 {tagCd: 'JWJT', uiTagNm: '전원주택'}, {tagCd: 'DDDGG', uiTagNm: '단독/다가구'}, {tagCd: 'SGJT', uiTagNm: '상가주택'},
                 {tagCd: 'HOJT', uiTagNm: '한옥주택'}, {tagCd: 'JGB', uiTagNm: '재개발'}, {tagCd: 'OR', uiTagNm: '원룸'},
                 {tagCd: 'GSW', uiTagNm: '고시원'}, {tagCd: 'SG', uiTagNm: '상가'}, {tagCd: 'SMS', uiTagNm: '사무실'},
                 {tagCd: 'GJCG', uiTagNm: '공장/창고'}, {tagCd: 'GM', uiTagNm: '건물'}, {tagCd: 'TJ', uiTagNm: '토지'},
                 {tagCd: 'APTHGJ', uiTagNm: '지식산업센터'}];
    
    출처: https://cocoabba.tistory.com/57 [새로운 시작~!:티스토리]
    ```
    
4. response json 데이터로 재구성
5. 결과 리스트 dataframe 형태로 변환 및 csv 파일로 저장

### 실행 코드(.py)

```python
import json
import pandas as pd
import math
import requests
from bs4 import BeautifulSoup

def main():
    # 결과 리스트 (크롤링한 json 데이터를 저장하고 있는 결과 리스트)
    result_list = []
    # 크롤링 할 지역 (네이버 접근 제한 이슈로 한 번에 1600개까지만 조회 가능 -> 적절히 수정해서 사용할 것)
    keywords = ["서초구"]
          # "광진구",
          # "성동구",
          # "성북구",
          # "동대문구",
          # "마포구",
          # "서대문구",
          # "은평구",
          # "강동구",
          # "송파구",
          # "관악구",
          # "동작구",
          # "서초구",
          # "영등포구",
          # "양천구",
          # "강서구",
          # "양천구",
          # "구로구",
          # "금천구",
          # "강남구",
          # "송파구",
          # "용산구",
          # "중구",
          # "종로구",
          # "중랑구",
          # "강북구",
          # "도봉구",
          # "노원구"

    for keyword in keywords:
        # 네이버 접근 제한 이슈로 접근 경로 우회를 위해 url, headers 설정
        url = f"https://m.land.naver.com/search/result/{keyword}"
        res = requests.get(url, headers={'User-agent':'Mozilla/5.0'})
        res.raise_for_status()
        soup = (str)(BeautifulSoup(res.text, "lxml"))

        # 해당 지역의 클러스터 리스트를 조회합니다.
        value = soup.split("filter: {")[1].split("}")[0].replace(" ", "").replace("'", "")

        lat = value.split("lat:")[1].split(",")[0]      # 위도
        lon = value.split("lon:")[1].split(",")[0]      # 경도
        z = value.split("z:")[1].split(",")[0]
        cortarNo = value.split("cortarNo:")[1].split(",")[0]
        rletTpCds = value.split("rletTpCds:")[1].split(",")[0]
        tradTpCds = value.split("tradTpCds:")[1].split()[0]

        # lat - btm : 37.550985 - 37.4331698 = 0.1178152
        # top - lat : 37.6686142 - 37.550985 = 0.1176292
        lat_margin = 0.118

        # lon - lft : 126.849534 - 126.7389841 = 0.1105499
        # rgt - lon : 126.9600839 - 126.849534 = 0.1105499
        lon_margin = 0.111

        btm = float(lat) - lat_margin
        lft = float(lon) - lon_margin
        top = float(lat) + lat_margin
        rgt = float(lon) + lon_margin

        rletTpCds = "APT:OPST:VL:OR:DDDGG"  # 아파트, 오피스텔, 빌라, 원룸, 단독/다가구 매물
        tradTpCds = "B2"  # 월세 매물

        wprcMax = 1000  # 보증금 1000만원 이하
        rprcMax = 70    # 월세 70만원 이하

        # 클러스터 리스트 그룹의 데이터를 가져옵니다.
        remaked_URL = f"https://m.land.naver.com/cluster/clusterList?view=atcl&cortarNo={cortarNo}&rletTpCd={rletTpCds}&tradTpCd={tradTpCds}&z={z}&lat={lat}&lon={lon}&btm={btm}&lft={lft}&top={top}&rgt={rgt}&wprcMax={wprcMax}&rprcMax={rprcMax}"

        res2 = requests.get(remaked_URL, headers={'User-agent':'Mozilla/5.0'})
        json_str = json.loads(json.dumps(res2.json()))
        values = json_str['data']['ARTICLE']

        # 큰 원으로 구성되어 있는 전체 매물그룹(values)을 load 하여 한 그룹씩 세부 쿼리를 진행합니다.
        for v in values:
            lgeo = v['lgeo']
            count = v['count']
            z2 = v['z']
            lat2 = v['lat']
            lon2 = v['lon']

            len_pages = count / 20 + 1
            for idx in range(1, math.ceil(len_pages)):
                remaked_URL2 = f"https://m.land.naver.com/cluster/ajax/articleList?itemId={lgeo}&mapKey=&lgeo={lgeo}&showR0=&rletTpCd={rletTpCds}&tradTpCd={tradTpCds}&z={z2}&lat={lat2}&lon={lon2}&totCnt={count}&cortarNo={cortarNo}&page={idx}"
                res3 = requests.get(remaked_URL2, headers={'User-agent':'Mozilla/5.0'})
                json_str = json.loads(json.dumps(res3.json()))
                atcls = json_str['body']
                for atcl in atcls:
                    result_list.append(atcl)

        # pandas 모듈을 이용해 크롤링한 결과를 dataframe 형식으로 변환합니다.
        df = pd.json_normalize(result_list)
        # csv 파일에 데이터를 저장합니다. (수정모드)
        df.to_csv('result_naver.csv', mode='a', header=False, index=False, encoding='UTF-8')

if __name__ == "__main__":
    main()

```

## trouble shooting

### 데이터 호출 개수 제한

- 네이버 측에서 크롤링 막아 둠
- 약 1600개 데이터 호출 시 제한 됨
- 한 번에 1600개 이내의 데이터까지만 가져오도록 함
    - 서울특별시 각 구 별로 나눠 각각 크롤링 함

### csv 파일 수정모드

- 각 구 별로 크롤링 해서 하나의 csv 파일에 저장할 때 덮어쓰기 문제
- 이전에 가져온 데이터가 사라졌음
- `to_csv` 함수 실행 시 `mode='a', header=False, index=False` 옵션 추가해 해결
    - `mode = 'a'` : 파일 수정모드로 파일을 엽니다. (덮어쓰기X)
    - `header`
        - `=True` : 데이터와 필드 값을 함께 저장합니다.
        - `=False` : 데이터 값만 저장합니다.
    - `index=False` : 데이터 추가 시 index를 저장하지 않습니다.
